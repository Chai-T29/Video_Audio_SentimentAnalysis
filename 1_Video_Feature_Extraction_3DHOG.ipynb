{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4c10d0-253c-4b41-a350-d95cc9f5284f",
   "metadata": {},
   "source": [
    "# Video Feature Extraction\n",
    "Now that we have our training and testing tensors, we can start extracting relevant features from the video data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1509cf-869e-48d8-8bc7-d81206b1a34f",
   "metadata": {},
   "source": [
    "### Importing Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43b03b6a-5d7f-4337-968e-9a43e8985948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "from scipy.signal import convolve2d\n",
    "import tensorly as tl\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b12884a8-e60d-4316-91a8-683e67a89e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 426, 3, 50, 960)\n",
      "(240, 426, 3, 50, 480)\n",
      "(960,)\n",
      "(480,)\n",
      "(960, 7)\n",
      "(480, 7)\n"
     ]
    }
   ],
   "source": [
    "train_test_data_dir = os.path.join(os.getcwd(), 'Train_Test_Data')\n",
    "train_videos = np.load(os.path.join(train_test_data_dir, 'train_videos.npy'))\n",
    "test_videos = np.load(os.path.join(train_test_data_dir, 'test_videos.npy'))\n",
    "y_train = np.load(os.path.join(train_test_data_dir, 'y_train.npy'))\n",
    "y_test = np.load(os.path.join(train_test_data_dir, 'y_test.npy'))\n",
    "id_train = np.load(os.path.join(train_test_data_dir, 'id_train.npy'))\n",
    "id_test = np.load(os.path.join(train_test_data_dir, 'id_test.npy'))\n",
    "\n",
    "print(train_videos.shape)\n",
    "print(test_videos.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(id_train.shape)\n",
    "print(id_test.shape)\n",
    "\n",
    "labels = ['Neutral', 'Calm', 'Happy', 'Sad', 'Angry', 'Fearful', 'Disgust', 'Surprised']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bee17d-70f1-4f1a-bc32-c44206cdf748",
   "metadata": {},
   "source": [
    "### Breakdown of Custom 3D Histogram of Oriented Gradients for Dimensionality Reduction\n",
    "\n",
    "The formulation of this Histogram of Oriented Gradients algorithm is loosely based off of recent research in the field of computer vision. However, this project translates this approach for 3 dimensions. Here's an overview of the methodology that is applied to both training and testing datasets:\n",
    "\n",
    "**1.** Iterate through every sample, convert the frames to grayscale, and (optionally) apply the following Gaussian filter to each frame of the video ($V$):\n",
    "\n",
    "$$Gaussian\\>Filter \\> \\Longrightarrow \\> \\frac{1}{1115}\\begin{pmatrix}\n",
    "  1 & 4 & 7 & 10 & 7 & 4 & 1 \\\\\n",
    "  4 & 12 & 26 & 33 & 26 & 12 & 4 \\\\\n",
    "  7 & 26 & 55 & 71 & 55 & 26 & 7 \\\\\n",
    "  10 & 33 & 71 & 91 & 71 & 33 & 10 \\\\\n",
    "  7 & 26 & 55 & 71 & 55 & 26 & 7 \\\\\n",
    "  4 & 12 & 26 & 33 & 26 & 12 & 4 \\\\\n",
    "  1 & 4 & 7 & 10 & 7 & 4 & 1\n",
    "\\end{pmatrix} \\\\\n",
    "$$\n",
    "\n",
    "**2.** We then compute the gradients with respect to the height, width [1], and frames ($\\frac{\\partial V}{\\partial x}$, $\\frac{\\partial V}{\\partial y}$, $\\frac{\\partial V}{\\partial z}$).\n",
    "\n",
    "**3.** Using these gradients, we can compute the three dimensional gradient magnitude for each video.\n",
    "$$\n",
    "G = \\sqrt{\\left( \\frac{\\partial V}{\\partial x}\\right)^2 + \\left( \\frac{\\partial V}{\\partial y} \\right)^2 + \\left( \\frac{\\partial V}{\\partial z}\\right)^2}\n",
    "$$\n",
    "\n",
    "**4.** Generally for images ($I$), we compute the gradient direction by $\\theta = \\arctan \\left( \\frac{\\frac{\\partial I}{\\partial y}}{\\frac{\\partial I}{\\partial x}} \\right)$ [2]. Since we have videos, we must compute the azimuthal angle and the polar angle to capture the 3D feature-space [4].\n",
    "\n",
    "$$\n",
    "\\theta_{azimuth} = \\arctan \\left( \\frac{\\frac{\\partial V}{\\partial y}}{\\frac{\\partial V}{\\partial x}} \\right) \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\phi_{polar} = \\arctan \\left( \\frac{\\sqrt{\\left( \\frac{\\partial V}{\\partial x}\\right)^2 + \\left( \\frac{\\partial V}{\\partial y} \\right)^2}}{\\frac{\\partial V}{\\partial z}} \\right) \\\\\n",
    "$$\n",
    "\n",
    "**5.** With our three sets of features, we can now partition the video into cells [3]. In our case, the cell size is ($5$, $6$, $5$), which will group sets of 180 pixels together.\n",
    "\n",
    "**6.** With our grouped pixels, we cluster the gradient magnitudes of each pixel ($G_{(i, j, k)}$) into bins based on the azimuthal and polar angles. With $9$ bins, we sum the gradient magnitudes for all the pixels belonging to each bin for both types of angles, which reduces the dimensionality from $180$ points in each cell to  $9*2 = 18$ points per cell. We can then save these results to disk.\n",
    "\n",
    "Example of HOG Features (in 2D):\n",
    "\n",
    "![HOG Features](https://ars.els-cdn.com/content/image/3-s2.0-B9780128149768000051-f05-13-9780128149768.jpg)\n",
    "\n",
    "Source: https://www.sciencedirect.com/topics/computer-science/histogram-of-oriented-gradient\n",
    "\n",
    "We follow a similar approach as outlined in the image above, but in 3D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace720f2-1ec4-48df-ae83-c5c97e1e18d3",
   "metadata": {},
   "source": [
    "#### Here's an example of what the gradient magnitude, azimuthal angle, and polar angle looks like for the videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4372769c-8b62-49cc-8159-a6697906dfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 72 #random video from training data\n",
    "\n",
    "gray_frames = tl.tenalg.mode_dot(train_videos[:, :, :, :, sample], np.array([0.2989, 0.5870, 0.1140]), mode=2)\n",
    "\n",
    "gx = np.gradient(gray_frames, axis=0)\n",
    "gy = np.gradient(gray_frames, axis=1)\n",
    "gz = np.gradient(gray_frames, axis=2)\n",
    "\n",
    "magnitude = np.sqrt(gx**2 + gy**2 + gz**2)\n",
    "azimuthal_angle = np.arctan2(gy, gx)\n",
    "polar_angle = np.arctan2(np.sqrt(gx**2, gy**2), gz)\n",
    "\n",
    "del gx, gy, gz\n",
    "gc.collect()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 7))\n",
    "axes=axes.flatten()\n",
    "\n",
    "im1 = axes[0].imshow(gray_frames[:, :, 0], cmap='gray')\n",
    "im2 = axes[1].imshow(magnitude[:, :, 0], cmap='gray')\n",
    "im3 = axes[2].imshow(azimuthal_angle[:, :, 0], cmap='gray')\n",
    "im4 = axes[3].imshow(polar_angle[:, :, 0], cmap='gray')\n",
    "\n",
    "axes[0].set_title('Grayscale Video')\n",
    "axes[1].set_title('Gradient Magnitude')\n",
    "axes[2].set_title('Azimuthal Angle')\n",
    "axes[3].set_title('Polar Angle')\n",
    "\n",
    "def update(i):\n",
    "    im1.set_array(gray_frames[:, :, i])\n",
    "    im2.set_array(magnitude[:, :, i])\n",
    "    im3.set_array(azimuthal_angle[:, :, i])\n",
    "    im4.set_array(polar_angle[:, :, i])\n",
    "    return [im1, im2, im3]\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.close(fig)\n",
    "\n",
    "fig.suptitle(f'Example features for {labels[y_train[sample]-1]} emotion')\n",
    "ani = FuncAnimation(fig, update, frames=gray_frames.shape[2]//2, blit=False, interval=100)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cae2cd3-bb7e-4f95-9196-3d6ccf95f46c",
   "metadata": {},
   "source": [
    "### Implementing Feature Extraction and Saving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "167e0a33-1580-4d52-a0d6-916a6f9ca79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_descriptor_shape(width, height, n_frames, cell_size, nbins):\n",
    "    \"\"\"Calculate the total shape of the descriptor.\"\"\"\n",
    "\n",
    "    cells_per_width = width // cell_size[0]\n",
    "    cells_per_height = height // cell_size[1]\n",
    "    cells_per_depth = n_frames // cell_size[2]\n",
    "\n",
    "    return cells_per_width, cells_per_height, cells_per_depth, nbins, 2\n",
    "\n",
    "def compute_hog3d_rgb(frames, cell_size, nbins, v, gaussian_filter):\n",
    "    \"\"\"Compute the HOG3D features for a video.\"\"\"\n",
    "\n",
    "    width, height, channels, n_frames = frames.shape\n",
    "    gray_frames = tl.tenalg.mode_dot(frames, np.array([0.2989, 0.5870, 0.1140]), mode=2)\n",
    "\n",
    "    if gaussian_filter:\n",
    "        print('Applying Gaussian Filter')\n",
    "        gaussian_filter = np.array([[1, 4, 7, 10, 7, 4, 1],\n",
    "                                [4, 12, 26, 33, 26, 12, 4],\n",
    "                                [7, 26, 55, 71, 55, 26, 7],\n",
    "                                [10, 33, 71, 91, 71, 33, 10],\n",
    "                                [7, 26, 55, 71, 55, 26, 7],\n",
    "                                [4, 12, 26, 33, 26, 12, 4],\n",
    "                                [1, 4, 7, 10, 7, 4, 1]]) / 1115\n",
    "\n",
    "\n",
    "\n",
    "        data = np.zeros_like(gray_frames)\n",
    "        for i in range(n_frames):\n",
    "            data[:, :, i] = convolve2d(gray_frames[:, :, i], gaussian_filter, mode='same')\n",
    "    else:\n",
    "        data = gray_frames\n",
    "        del gray_frames\n",
    "        gc.collect()\n",
    "\n",
    "    gx = np.gradient(data, axis=0)\n",
    "    gy = np.gradient(data, axis=1)\n",
    "    gz = np.gradient(data, axis=2)\n",
    "\n",
    "    magnitude = np.sqrt(gx**2 + gy**2 + gz**2)\n",
    "    azimuthal_angle = np.arctan2(gy, gx)\n",
    "    polar_angle = np.arctan2(np.sqrt(gx**2, gy**2), gz)\n",
    "\n",
    "    output_shape = calculate_total_descriptor_shape(width, height, n_frames, cell_size, nbins)\n",
    "    hog3d_descriptors = np.zeros(output_shape, dtype=np.float32)\n",
    "\n",
    "    for i in range(0, n_frames - cell_size[2], cell_size[2]):\n",
    "        for y in range(0, height - cell_size[1], cell_size[1]):\n",
    "            for x in range(0, width - cell_size[0], cell_size[0]):\n",
    "                cell_magnitude = magnitude[x:x + cell_size[0], y:y + cell_size[1], i:i + cell_size[2]]\n",
    "                cell_azimuthal = azimuthal_angle[x:x + cell_size[0], y:y + cell_size[1], i:i + cell_size[2]]\n",
    "                cell_polar = polar_angle[x:x + cell_size[0], y:y + cell_size[1], i:i + cell_size[2]]\n",
    "\n",
    "                hist_azimuthal, _ = np.histogram(cell_azimuthal, bins=nbins, range=(-np.pi, np.pi), weights=cell_magnitude)\n",
    "                hist_polar, _ = np.histogram(cell_polar, bins=nbins, range=(0, np.pi), weights=cell_magnitude)\n",
    "\n",
    "                hist_azimuthal = hist_azimuthal / (np.linalg.norm(hist_azimuthal) + 1e-6)\n",
    "                hist_polar = hist_polar / (np.linalg.norm(hist_polar) + 1e-6)\n",
    "\n",
    "                hog3d_descriptors[x//cell_size[0], y//cell_size[1], i//cell_size[2], :, 0] = hist_azimuthal\n",
    "                hog3d_descriptors[x//cell_size[0], y//cell_size[1], i//cell_size[2], :, 1] = hist_polar\n",
    "\n",
    "    del cell_magnitude, cell_azimuthal, cell_polar, hist_azimuthal, hist_polar, gx, gy, gz, magnitude, azimuthal_angle, polar_angle\n",
    "    gc.collect()\n",
    "\n",
    "    return hog3d_descriptors\n",
    "\n",
    "def process_videos(videos, cell_size=(5, 6, 5), nbins=9, gaussian_filter=False):\n",
    "    \"\"\"Process a batch of videos.\"\"\"\n",
    "\n",
    "    width, height, channels, n_frames, video_count = videos.shape\n",
    "\n",
    "    for v in tqdm(range(video_count)):\n",
    "        hog3d_descriptors = compute_hog3d_rgb(videos[:, :, :, :, v].astype(np.float32), cell_size, nbins, v, gaussian_filter)\n",
    "        if v == 0:\n",
    "            descriptors_shape = hog3d_descriptors.shape\n",
    "            all_descriptors = np.zeros((video_count, *descriptors_shape), dtype=np.float32)\n",
    "\n",
    "        all_descriptors[v] += hog3d_descriptors\n",
    "\n",
    "        del hog3d_descriptors\n",
    "        gc.collect()\n",
    "\n",
    "    return all_descriptors\n",
    "\n",
    "def save_to_disk(variable, filename):\n",
    "    np.save(filename, variable)\n",
    "    del variable\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e08a832-ece7-42bf-815a-a2bf9008cf66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2070ca565e41eabefa6b8ab500a36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/960 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc74d097038642839f49ef6c8885b7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960, 48, 71, 10, 9, 2)\n",
      "(480, 48, 71, 10, 9, 2)\n"
     ]
    }
   ],
   "source": [
    "H_train = process_videos(train_videos)\n",
    "H_test = process_videos(test_videos)\n",
    "\n",
    "print(H_train.shape)\n",
    "print(H_test.shape)\n",
    "\n",
    "hog3d_data_dir = os.path.join(os.getcwd(), 'HOG3d_Data')\n",
    "os.makedirs(hog3d_data_dir, exist_ok=True)\n",
    "\n",
    "save_to_disk(H_train, os.path.join(hog3d_data_dir, 'H_train.npy'))\n",
    "save_to_disk(H_test, os.path.join(hog3d_data_dir, 'H_test.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d69260e-5836-45cd-aa83-71860c019883",
   "metadata": {},
   "source": [
    "### References:\n",
    "[1] SpringerLink (Online service), Panigrahi, C. R., Pati, B., Mohapatra, P., Buyya, R., & Li, K. (2021). Progress in Advanced Computing and Intelligent Engineering: Proceedings of ICACIE 2019, Volume 1 (1st ed. 2021.). Springer Singapore : Imprint: Springer. https://doi.org/10.1007/978-981-15-6584-7\n",
    "\n",
    "[2] Zoubir, Hajar & Rguig, Mustapha & Aroussi, Mohamed & Chehri, Abdellah & Rachid, Saadane. (2022). Concrete Bridge Crack Image Classification Using Histograms of Oriented Gradients, Uniform Local Binary Patterns, and Kernel Principal Component Analysis. Electronics. 11. 3357. 10.3390/electronics11203357. https://doi.org/10.3390/electronics11203357\n",
    "\n",
    "[3]  S V Shidlovskiy et al 2020 J. Phys.: Conf. Ser. 1611 012072 https://doi:10.1088/1742-6596/1611/1/012072\n",
    "\n",
    "[4] Nykamp DQ, “Spherical coordinates.” From Math Insight. http://mathinsight.org/spherical_coordinates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
